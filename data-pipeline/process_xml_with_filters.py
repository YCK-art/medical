"""
í•„í„°ë§ ê¸°ëŠ¥ì´ í†µí•©ëœ XML ë…¼ë¬¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

ì›Œí¬í”Œë¡œìš°:
1. XML í´ë” ìŠ¤ìº”
2. ì¤‘ë³µ ë…¼ë¬¸ í•„í„°ë§ (ì´ë¯¸ Pineconeì— ìˆëŠ” ë…¼ë¬¸ ì œì™¸)
3. CC-BY ë¼ì´ì„ ìŠ¤ í™•ì¸
4. í†µê³¼í•œ ë…¼ë¬¸ë§Œ ì²­í‚¹ & ì„ë² ë”© & Pinecone ì €ì¥

ì‚¬ìš©ë²•:
    python3 process_xml_with_filters.py \\
        --xml-folder /path/to/xmls \\
        --journal "Journal Name" \\
        --progress-file progress.json
"""

import os
import re
import json
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Dict, Optional, Set
from dotenv import load_dotenv
from openai import OpenAI
from pinecone import Pinecone
import sys
import argparse
import random

load_dotenv()

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index = pc.Index("medical-guidelines")


# ============================================================
# Step 0: í•„í„°ë§ í•¨ìˆ˜ë“¤
# ============================================================

def extract_pmcid_from_xml(xml_path: Path) -> Optional[str]:
    """XMLì—ì„œ PMCID ì¶”ì¶œ"""
    try:
        pmcid_match = re.search(r'PMC\d+', xml_path.name)
        if pmcid_match:
            return pmcid_match.group(0)

        tree = ET.parse(xml_path)
        root = tree.getroot()
        article_meta = root.find('.//article-meta')
        if article_meta is not None:
            pmcid_elem = article_meta.find('.//article-id[@pub-id-type="pmc"]')
            if pmcid_elem is not None:
                return f"PMC{pmcid_elem.text.strip()}"
        return None
    except:
        return None


def extract_license_from_xml(xml_path: Path) -> Optional[str]:
    """XMLì—ì„œ ë¼ì´ì„ ìŠ¤ ì¶”ì¶œ"""
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()

        for license_elem in root.iter('license'):
            # license-p íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for license_p in license_elem.iter('license-p'):
                text = ''.join(license_p.itertext())

                # Creative Commons URL íŒŒì‹±
                cc_match = re.search(r'creativecommons\.org/licenses/([\w-]+)/', text)
                if cc_match:
                    license_code = cc_match.group(1).upper()
                    return f"CC-{license_code}"

                # í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ CC-BY ë“± ì°¾ê¸°
                text_upper = text.upper()
                if "CC BY-NC" in text_upper or "CC-BY-NC" in text_upper:
                    return "CC-BY-NC"
                elif "CC BY" in text_upper or "CC-BY" in text_upper:
                    return "CC-BY"

            # ext-link íƒœê·¸ì—ì„œ URL ì°¾ê¸°
            for ext_link in license_elem.iter('ext-link'):
                href = ext_link.get('href', '') or ext_link.get('{http://www.w3.org/1999/xlink}href', '')
                if href and 'creativecommons.org/licenses/' in href:
                    cc_match = re.search(r'creativecommons\.org/licenses/([\w-]+)', href)
                    if cc_match:
                        license_code = cc_match.group(1).upper()
                        return f"CC-{license_code}"

        return None
    except Exception as e:
        return None


def get_existing_pmcids(journal_name: str = None) -> Set[str]:
    """Pineconeì—ì„œ ì´ë¯¸ í•™ìŠµëœ PMCID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    existing_pmcids = set()

    for i in range(20):
        random_vector = [random.uniform(-1, 1) for _ in range(1536)]
        filter_dict = {"journal": {"$eq": journal_name}} if journal_name else None

        results = index.query(
            vector=random_vector,
            top_k=10000,
            include_metadata=True,
            filter=filter_dict
        )

        for match in results.matches:
            pmcid = match.metadata.get("pmcid", "")
            if pmcid:
                existing_pmcids.add(pmcid)

        if i > 5 and len(results.matches) == 0:
            break

    return existing_pmcids


def filter_xmls(xml_files: List[Path], journal_name: str = None) -> Dict:
    """
    XML íŒŒì¼ë“¤ í•„í„°ë§

    Returns:
        {
            "valid": [Path, ...],  # í•™ìŠµ ê°€ëŠ¥
            "duplicate": [Path, ...],  # ì¤‘ë³µ
            "non_cc_by": [Path, ...],  # ë¹„ CC-BY
            "no_license": [Path, ...]  # ë¼ì´ì„ ìŠ¤ ë¶ˆëª…
        }
    """
    print()
    print("="*80)
    print("ğŸ” XML íŒŒì¼ í•„í„°ë§ ì‹œì‘")
    print("="*80)
    print()

    # 1ë‹¨ê³„: ê¸°ì¡´ PMCID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    print("ğŸ“Š 1ë‹¨ê³„: ì¤‘ë³µ ë…¼ë¬¸ í™•ì¸ ì¤‘...")
    existing_pmcids = get_existing_pmcids(journal_name)
    print(f"   ê¸°ì¡´ ë…¼ë¬¸: {len(existing_pmcids):,}ê°œ ë°œê²¬")
    print()

    # 2ë‹¨ê³„: ê° XML íŒŒì¼ ê²€ì‚¬
    print("ğŸ“Š 2ë‹¨ê³„: ê° XML íŒŒì¼ ê²€ì‚¬ ì¤‘...")

    valid_xmls = []
    duplicate_xmls = []
    non_cc_by_xmls = []
    no_license_xmls = []

    for idx, xml_file in enumerate(xml_files, 1):
        if idx % 50 == 0:
            print(f"   ì§„í–‰: {idx}/{len(xml_files)}...")

        # PMCID ì¶”ì¶œ
        pmcid = extract_pmcid_from_xml(xml_file)

        # ì¤‘ë³µ ì²´í¬
        if pmcid and pmcid in existing_pmcids:
            duplicate_xmls.append(xml_file)
            continue

        # ë¼ì´ì„ ìŠ¤ ì²´í¬
        license_info = extract_license_from_xml(xml_file)

        if license_info == "CC-BY":
            valid_xmls.append(xml_file)
        elif license_info and "NC" in license_info:
            non_cc_by_xmls.append(xml_file)
        elif license_info is None:
            no_license_xmls.append(xml_file)
        else:
            valid_xmls.append(xml_file)  # ë‹¤ë¥¸ CC ë¼ì´ì„ ìŠ¤ëŠ” ì¼ë‹¨ í—ˆìš©

    print()
    print("="*80)
    print("ğŸ“Š í•„í„°ë§ ê²°ê³¼")
    print("="*80)
    print(f"âœ… í•™ìŠµ ê°€ëŠ¥ (CC-BY):          {len(valid_xmls):,}ê°œ")
    print(f"âŒ ì¤‘ë³µ (ì´ë¯¸ í•™ìŠµë¨):         {len(duplicate_xmls):,}ê°œ")
    print(f"âš ï¸  ë¹„-CC-BY (ìƒì—…ì  ë¶ˆê°€):    {len(non_cc_by_xmls):,}ê°œ")
    print(f"â“ ë¼ì´ì„ ìŠ¤ ë¶ˆëª…:              {len(no_license_xmls):,}ê°œ")
    print("="*80)
    print()

    return {
        "valid": valid_xmls,
        "duplicate": duplicate_xmls,
        "non_cc_by": non_cc_by_xmls,
        "no_license": no_license_xmls
    }


# ============================================================
# Step 1~4: ê¸°ì¡´ í”„ë¡œì„¸ì‹± í•¨ìˆ˜ë“¤ (process_frontvet_xml.pyì™€ ë™ì¼)
# ============================================================

def extract_text_from_element(element, default=""):
    """XML ì—˜ë¦¬ë¨¼íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if element is None:
        return default
    text = ''.join(element.itertext()).strip()
    return text if text else default


def extract_xml_metadata(xml_path: Path) -> Dict:
    """PMC XMLì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()

        metadata = {
            "title": "",
            "authors": "",
            "journal": "",
            "year": "",
            "doi": "",
            "pmcid": "",
            "pmid": "",
            "abstract": ""
        }

        # PMCID ì¶”ì¶œ
        pmcid_match = re.search(r'PMC\d+', xml_path.name)
        if pmcid_match:
            metadata["pmcid"] = pmcid_match.group(0)

        article_meta = root.find('.//article-meta')
        if article_meta is None:
            return metadata

        # ì œëª©
        title_elem = article_meta.find('.//article-title')
        if title_elem is not None:
            metadata["title"] = extract_text_from_element(title_elem)

        # ì €ì
        authors = []
        for contrib in article_meta.findall('.//contrib[@contrib-type="author"]'):
            name_elem = contrib.find('.//name')
            if name_elem is not None:
                surname = extract_text_from_element(name_elem.find('surname'))
                given = extract_text_from_element(name_elem.find('given-names'))
                if surname:
                    author_name = f"{given} {surname}" if given else surname
                    authors.append(author_name)

        if authors:
            if len(authors) <= 6:
                metadata["authors"] = ", ".join(authors)
            else:
                metadata["authors"] = ", ".join(authors[:6]) + ", et al."

        # ì €ë„ëª…
        journal_elem = root.find('.//journal-title')
        if journal_elem is not None:
            metadata["journal"] = extract_text_from_element(journal_elem)

        # ì—°ë„
        year_elem = article_meta.find('.//pub-date[@pub-type="epub"]/year')
        if year_elem is None:
            year_elem = article_meta.find('.//pub-date/year')
        if year_elem is not None:
            metadata["year"] = extract_text_from_element(year_elem)

        # DOI
        doi_elem = article_meta.find('.//article-id[@pub-id-type="doi"]')
        if doi_elem is not None:
            metadata["doi"] = extract_text_from_element(doi_elem)

        # PMID
        pmid_elem = article_meta.find('.//article-id[@pub-id-type="pmid"]')
        if pmid_elem is not None:
            metadata["pmid"] = extract_text_from_element(pmid_elem)

        return metadata

    except Exception as e:
        print(f"  âŒ XML íŒŒì‹± ì˜¤ë¥˜: {e}")
        return {}


def extract_xml_body_text(xml_path: Path) -> str:
    """PMC XMLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()

        body_elem = root.find('.//body')
        if body_elem is None:
            return ""

        body_text = extract_text_from_element(body_elem)
        return body_text

    except Exception as e:
        return ""


def clean_xml_text(text: str) -> str:
    """XML í…ìŠ¤íŠ¸ ì •ë¦¬"""
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r' {2,}', ' ', text)
    text = text.replace('\u200b', '')
    text = text.replace('\xa0', ' ')
    return text.strip()


def recursive_chunk_with_overlap(text: str, chunk_size: int = 600, overlap: int = 150) -> List[str]:
    """Recursive Chunking with Overlap"""
    chunks = []
    start = 0

    while start < len(text):
        end = min(start + chunk_size, len(text))

        if end < len(text):
            found_separator = False
            for separator in ['. ', '.\n', '\n\n', '\n', ' ']:
                last_sep = text.rfind(separator, start, end)
                if last_sep > start:
                    end = last_sep + len(separator)
                    found_separator = True
                    break

            if not found_separator:
                end = start + chunk_size

        chunk = text[start:end].strip()
        if chunk and len(chunk) > 50:
            chunks.append(chunk)

        next_start = end - overlap
        if next_start <= start:
            next_start = start + chunk_size

        start = next_start

        if start >= len(text):
            break

    return chunks


def create_embeddings(texts: List[str], batch_size: int = 100) -> List[List[float]]:
    """OpenAI APIë¡œ ì„ë² ë”© ìƒì„±"""
    all_embeddings = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]

        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=batch
        )

        batch_embeddings = [item.embedding for item in response.data]
        all_embeddings.extend(batch_embeddings)

        print(f"  ğŸ“Š ì„ë² ë”© ìƒì„±: {i+1}-{i+len(batch)}/{len(texts)}")

    return all_embeddings


def upsert_to_pinecone(chunks_metadata: List[Dict], embeddings: List[List[float]], batch_size: int = 100):
    """Pineconeì— ë²¡í„° ì €ì¥"""
    total = len(chunks_metadata)

    for i in range(0, total, batch_size):
        batch_meta = chunks_metadata[i:i + batch_size]
        batch_emb = embeddings[i:i + batch_size]

        vectors = []
        for chunk_meta, embedding in zip(batch_meta, batch_emb):
            metadata = {
                "doc_type": "paper",
                "title": chunk_meta["title"],
                "year": chunk_meta["year"],
                "page": chunk_meta.get("chunk_index", 0),
                "text": chunk_meta["text"],
                "reference_format": chunk_meta["reference_format"],
                "authors": chunk_meta.get("authors", ""),
                "journal": chunk_meta.get("journal", ""),
                "doi": chunk_meta.get("doi", ""),
                "pmcid": chunk_meta.get("pmcid", ""),
                "pmid": chunk_meta.get("pmid", "")
            }

            vectors.append({
                "id": chunk_meta["id"],
                "values": embedding,
                "metadata": metadata
            })

        index.upsert(vectors=vectors)
        print(f"  ğŸ’¾ Pinecone ì €ì¥: {i+1}-{i+len(batch_meta)}/{total}")


def process_single_xml(xml_path: Path) -> Dict:
    """ë‹¨ì¼ XML íŒŒì¼ ì²˜ë¦¬"""
    try:
        print(f"\n{'='*60}")
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {xml_path.name}")
        print(f"{'='*60}")
        sys.stdout.flush()

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = extract_xml_metadata(xml_path)

        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        body_text = extract_xml_body_text(xml_path)

        if not body_text or len(body_text) < 100:
            print(f"  âš ï¸  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"success": False, "error": "ë³¸ë¬¸ ì—†ìŒ"}

        # í…ìŠ¤íŠ¸ ì •ë¦¬
        clean_text = clean_xml_text(body_text)

        # ì²­í¬ ë¶„í• 
        chunks = recursive_chunk_with_overlap(clean_text, chunk_size=600, overlap=150)
        print(f"  ğŸ“¦ ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        # ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        all_chunks_metadata = []

        for chunk_idx, chunk_text in enumerate(chunks):
            pmcid = metadata.get('pmcid', xml_path.stem)
            chunk_id = f"paper_{pmcid}_c{chunk_idx}"
            chunk_id = re.sub(r'[^a-zA-Z0-9_-]', '_', chunk_id)

            ref_parts = []
            if metadata.get('authors'):
                ref_parts.append(metadata['authors'])
            if metadata.get('journal'):
                ref_parts.append(metadata['journal'])
            if metadata.get('year'):
                ref_parts.append(metadata['year'])

            ref_format = ". ".join(ref_parts) if ref_parts else metadata.get('title', '')[:50]
            if metadata.get('doi'):
                ref_format += f". doi:{metadata['doi']}"

            chunk_meta = {
                "id": chunk_id,
                "title": metadata.get("title", ""),
                "year": metadata.get("year", ""),
                "authors": metadata.get("authors", ""),
                "journal": metadata.get("journal", ""),
                "doi": metadata.get("doi", ""),
                "pmcid": metadata.get("pmcid", ""),
                "pmid": metadata.get("pmid", ""),
                "chunk_index": chunk_idx,
                "text": chunk_text,
                "reference_format": ref_format
            }

            all_chunks_metadata.append(chunk_meta)

        # ì„ë² ë”© ìƒì„±
        chunk_texts = [c["text"] for c in all_chunks_metadata]
        embeddings = create_embeddings(chunk_texts)

        # Pineconeì— ì €ì¥
        upsert_to_pinecone(all_chunks_metadata, embeddings)

        print(f"\n  âœ… ì™„ë£Œ!")
        sys.stdout.flush()

        return {
            "success": True,
            "chunks": len(all_chunks_metadata),
            "metadata": metadata
        }

    except Exception as e:
        print(f"\n  âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="í•„í„°ë§ ê¸°ëŠ¥ í†µí•© XML í”„ë¡œì„¸ì‹±")
    parser.add_argument("--xml-folder", required=True, help="XML íŒŒì¼ í´ë”")
    parser.add_argument("--journal", default=None, help="ì €ë„ ì´ë¦„")
    parser.add_argument("--progress-file", default="processing_progress.json", help="ì§„í–‰ ìƒí™© íŒŒì¼")

    args = parser.parse_args()

    xml_folder = Path(args.xml_folder)
    progress_file = Path(args.progress_file)

    if not xml_folder.exists():
        print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {xml_folder}")
        exit(1)

    print("="*80)
    print("ğŸš€ í•„í„°ë§ í†µí•© XML í”„ë¡œì„¸ì‹± ì‹œì‘")
    print("="*80)
    print(f"ğŸ“ í´ë”: {xml_folder}")
    print(f"ğŸ“° ì €ë„: {args.journal or 'ì „ì²´'}")
    print("="*80)

    # XML íŒŒì¼ ëª©ë¡
    xml_files = list(xml_folder.glob("*.xml"))
    xml_files = [f for f in xml_files if not f.name.startswith(".")]

    print(f"\nğŸ“Š ì „ì²´ XML íŒŒì¼: {len(xml_files)}ê°œ")

    # í•„í„°ë§
    filtered = filter_xmls(xml_files, args.journal)

    # ë¼ì´ì„ ìŠ¤ ë¶ˆëª… íŒŒì¼ ì²˜ë¦¬ ì—¬ë¶€ ë¬¼ì–´ë³´ê¸°
    if len(filtered["no_license"]) > 0:
        print(f"\nâš ï¸  ë¼ì´ì„ ìŠ¤ ë¶ˆëª… íŒŒì¼ {len(filtered['no_license'])}ê°œ ë°œê²¬")
        response = input("ë¼ì´ì„ ìŠ¤ ë¶ˆëª… íŒŒì¼ë„ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): ")
        if response.lower() == "yes":
            filtered["valid"].extend(filtered["no_license"])
            print(f"âœ… ë¼ì´ì„ ìŠ¤ ë¶ˆëª… íŒŒì¼ {len(filtered['no_license'])}ê°œ ì¶”ê°€")

    # ì§„í–‰ ìƒí™© ë¡œë“œ
    if progress_file.exists():
        with open(progress_file, 'r', encoding='utf-8') as f:
            progress = json.load(f)
    else:
        progress = {
            "processed_files": [],
            "total_processed": 0,
            "total_chunks": 0
        }

    processed_set = set(progress["processed_files"])

    # í•„í„°ë§ëœ íŒŒì¼ ì¤‘ ë¯¸ì²˜ë¦¬ íŒŒì¼ë§Œ
    to_process = [f for f in filtered["valid"] if f.name not in processed_set]

    print(f"\nğŸ“Š ì²˜ë¦¬ ëŒ€ìƒ: {len(to_process)}ê°œ íŒŒì¼")

    if len(to_process) == 0:
        print("âœ… ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        exit(0)

    # ì²˜ë¦¬ ì‹œì‘
    for idx, xml_file in enumerate(to_process, 1):
        print(f"\n[{idx}/{len(to_process)}] ì²˜ë¦¬ ì¤‘...")

        result = process_single_xml(xml_file)

        if result["success"]:
            progress["processed_files"].append(xml_file.name)
            progress["total_processed"] += 1
            progress["total_chunks"] += result["chunks"]

            # 10ê°œë§ˆë‹¤ ì €ì¥
            if progress["total_processed"] % 10 == 0:
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress, f, indent=2, ensure_ascii=False)
                print(f"\n  ğŸ’¾ ì§„í–‰ ìƒí™© ì €ì¥: {progress['total_processed']}ê°œ")

    # ìµœì¢… ì €ì¥
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress, f, indent=2, ensure_ascii=False)

    print(f"\n{'='*80}")
    print("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"ì´ ì²˜ë¦¬: {progress['total_processed']}ê°œ íŒŒì¼")
    print(f"ì´ ì²­í¬: {progress['total_chunks']:,}ê°œ")
    print(f"{'='*80}")
