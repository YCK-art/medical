"""
Pinecone DBì— ì´ë¯¸ í•™ìŠµëœ ë…¼ë¬¸ì„ í•„í„°ë§í•˜ì—¬ ìƒˆë¡œìš´ XMLë§Œ ì¶”ì¶œ

ì‚¬ìš©ë²•:
    python3 filter_duplicate_papers.py --xml-folder /path/to/xml --journal "Journal Name"
"""
import os
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Set, Dict
from dotenv import load_dotenv
from pinecone import Pinecone
import re
import argparse

load_dotenv()

pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index = pc.Index("medical-guidelines")


def extract_pmcid_from_xml(xml_path: Path) -> str:
    """XML íŒŒì¼ì—ì„œ PMCID ì¶”ì¶œ"""
    try:
        # íŒŒì¼ëª…ì—ì„œ PMCID ì¶”ì¶œ (ì˜ˆ: PMC1234567.xml)
        pmcid_match = re.search(r'PMC\d+', xml_path.name)
        if pmcid_match:
            return pmcid_match.group(0)

        # XML ë‚´ìš©ì—ì„œ PMCID ì¶”ì¶œ
        tree = ET.parse(xml_path)
        root = tree.getroot()

        article_meta = root.find('.//article-meta')
        if article_meta is not None:
            pmcid_elem = article_meta.find('.//article-id[@pub-id-type="pmc"]')
            if pmcid_elem is not None:
                return f"PMC{pmcid_elem.text.strip()}"

        return None
    except Exception as e:
        print(f"  âš ï¸  PMCID ì¶”ì¶œ ì˜¤ë¥˜ ({xml_path.name}): {e}")
        return None


def extract_doi_from_xml(xml_path: Path) -> str:
    """XML íŒŒì¼ì—ì„œ DOI ì¶”ì¶œ"""
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()

        article_meta = root.find('.//article-meta')
        if article_meta is not None:
            doi_elem = article_meta.find('.//article-id[@pub-id-type="doi"]')
            if doi_elem is not None:
                return doi_elem.text.strip()

        return None
    except Exception as e:
        return None


def get_existing_papers_from_pinecone(journal_name: str = None) -> Set[str]:
    """
    Pinecone DBì—ì„œ ì´ë¯¸ í•™ìŠµëœ ë…¼ë¬¸ ëª©ë¡ ì¶”ì¶œ (PMCID ê¸°ì¤€)

    Args:
        journal_name: íŠ¹ì • ì €ë„ë§Œ í•„í„°ë§ (Noneì´ë©´ ì „ì²´)

    Returns:
        ì´ë¯¸ í•™ìŠµëœ PMCID ì§‘í•©
    """
    print(f"ğŸ” Pinecone DBì—ì„œ ê¸°ì¡´ ë…¼ë¬¸ ëª©ë¡ ì¶”ì¶œ ì¤‘...")
    if journal_name:
        print(f"   ì €ë„: {journal_name}")

    existing_pmcids = set()
    existing_dois = set()

    # ì—¬ëŸ¬ ë²ˆ ì¿¼ë¦¬í•˜ì—¬ ì¶©ë¶„í•œ ìƒ˜í”Œ í™•ë³´
    import random

    for i in range(20):  # ìµœëŒ€ 200,000ê°œ ìƒ˜í”Œ
        random_vector = [random.uniform(-1, 1) for _ in range(1536)]

        filter_dict = {"journal": {"$eq": journal_name}} if journal_name else None

        results = index.query(
            vector=random_vector,
            top_k=10000,
            include_metadata=True,
            filter=filter_dict
        )

        for match in results.matches:
            meta = match.metadata
            pmcid = meta.get("pmcid", "")
            doi = meta.get("doi", "")

            if pmcid:
                existing_pmcids.add(pmcid)
            if doi:
                existing_dois.add(doi)

        print(f"   ì¿¼ë¦¬ {i+1}/20: {len(existing_pmcids):,}ê°œ PMCID, {len(existing_dois):,}ê°œ DOI ë°œê²¬")

        # ì¶©ë¶„íˆ ìˆ˜ì§‘ë˜ë©´ ì¤‘ë‹¨
        if i > 5 and len(results.matches) == 0:
            break

    print(f"âœ… ê¸°ì¡´ ë…¼ë¬¸: {len(existing_pmcids):,}ê°œ (PMCID ê¸°ì¤€)")
    return existing_pmcids, existing_dois


def filter_new_xmls(xml_folder: Path, journal_name: str = None) -> Dict:
    """
    ìƒˆë¡œìš´ XML íŒŒì¼ë§Œ í•„í„°ë§

    Returns:
        {
            "new_xmls": [...],
            "duplicate_xmls": [...],
            "no_pmcid_xmls": [...]
        }
    """
    print()
    print("="*80)
    print("ğŸ“‚ XML íŒŒì¼ í•„í„°ë§")
    print("="*80)

    # ê¸°ì¡´ ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    existing_pmcids, existing_dois = get_existing_papers_from_pinecone(journal_name)

    # XML íŒŒì¼ ëª©ë¡
    xml_files = list(xml_folder.glob("*.xml"))
    xml_files = [f for f in xml_files if not f.name.startswith(".")]

    print(f"\nğŸ“ XML í´ë”: {xml_folder}")
    print(f"ğŸ“Š ì „ì²´ XML íŒŒì¼: {len(xml_files)}ê°œ")
    print()

    new_xmls = []
    duplicate_xmls = []
    no_pmcid_xmls = []

    for xml_file in xml_files:
        pmcid = extract_pmcid_from_xml(xml_file)
        doi = extract_doi_from_xml(xml_file)

        if pmcid:
            if pmcid in existing_pmcids:
                duplicate_xmls.append({
                    "file": xml_file,
                    "pmcid": pmcid,
                    "doi": doi
                })
            else:
                new_xmls.append({
                    "file": xml_file,
                    "pmcid": pmcid,
                    "doi": doi
                })
        elif doi:
            if doi in existing_dois:
                duplicate_xmls.append({
                    "file": xml_file,
                    "pmcid": pmcid,
                    "doi": doi
                })
            else:
                new_xmls.append({
                    "file": xml_file,
                    "pmcid": pmcid,
                    "doi": doi
                })
        else:
            no_pmcid_xmls.append({
                "file": xml_file,
                "pmcid": None,
                "doi": None
            })

    # ê²°ê³¼ ì¶œë ¥
    print("="*80)
    print("ğŸ“Š í•„í„°ë§ ê²°ê³¼")
    print("="*80)
    print(f"âœ… ìƒˆë¡œìš´ ë…¼ë¬¸ (í•™ìŠµ ê°€ëŠ¥): {len(new_xmls):,}ê°œ")
    print(f"âŒ ì¤‘ë³µ ë…¼ë¬¸ (ì´ë¯¸ í•™ìŠµë¨): {len(duplicate_xmls):,}ê°œ")
    print(f"âš ï¸  PMCID/DOI ì—†ìŒ: {len(no_pmcid_xmls):,}ê°œ")
    print("="*80)

    return {
        "new_xmls": new_xmls,
        "duplicate_xmls": duplicate_xmls,
        "no_pmcid_xmls": no_pmcid_xmls
    }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ì¤‘ë³µ ë…¼ë¬¸ í•„í„°ë§")
    parser.add_argument("--xml-folder", required=True, help="XML íŒŒì¼ í´ë” ê²½ë¡œ")
    parser.add_argument("--journal", default=None, help="ì €ë„ ì´ë¦„ (ì„ íƒ)")

    args = parser.parse_args()

    xml_folder = Path(args.xml_folder)

    if not xml_folder.exists():
        print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {xml_folder}")
        exit(1)

    result = filter_new_xmls(xml_folder, args.journal)

    # ìƒˆë¡œìš´ XML íŒŒì¼ ëª©ë¡ ì¶œë ¥
    print()
    print("âœ… ìƒˆë¡œìš´ XML íŒŒì¼ ëª©ë¡ (ìƒ˜í”Œ 10ê°œ):")
    for item in result["new_xmls"][:10]:
        print(f"   {item['file'].name} (PMCID: {item['pmcid']})")

    if len(result["new_xmls"]) > 10:
        print(f"   ... ì™¸ {len(result['new_xmls']) - 10}ê°œ")

    print()
    print("âŒ ì¤‘ë³µ XML íŒŒì¼ ëª©ë¡ (ìƒ˜í”Œ 10ê°œ):")
    for item in result["duplicate_xmls"][:10]:
        print(f"   {item['file'].name} (PMCID: {item['pmcid']}) - ì´ë¯¸ í•™ìŠµë¨")

    if len(result["duplicate_xmls"]) > 10:
        print(f"   ... ì™¸ {len(result['duplicate_xmls']) - 10}ê°œ")

    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    import json
    output_file = xml_folder / "filtering_result.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "new_xmls": [str(item["file"]) for item in result["new_xmls"]],
            "duplicate_xmls": [str(item["file"]) for item in result["duplicate_xmls"]],
            "no_pmcid_xmls": [str(item["file"]) for item in result["no_pmcid_xmls"]]
        }, f, indent=2, ensure_ascii=False)

    print()
    print(f"ğŸ’¾ í•„í„°ë§ ê²°ê³¼ ì €ì¥: {output_file}")
